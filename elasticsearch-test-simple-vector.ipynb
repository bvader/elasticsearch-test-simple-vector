{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bvader/elasticsearch-test-simple-vector/blob/main/elasticsearch-test-simple-vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "ObVXyocDZSZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install elasticsearch\n"
      ],
      "metadata": {
        "id": "cRUzDQ2MtIna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in connection and auth info\n",
        "# Note the port is REQUIRED for the elasticsearch endpoint!\n",
        "import getpass, os\n",
        "\n",
        "os.environ['es_url'] = getpass.getpass('Enter Elasticsearch Endpoint:  ')\n",
        "os.environ['es_user'] = getpass.getpass('Enter User:  ')\n",
        "os.environ['es_pwd'] = getpass.getpass('Enter Password:  ')"
      ],
      "metadata": {
        "id": "Y3umJ1oqT4SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbqVOuPbsxb_"
      },
      "outputs": [],
      "source": [
        "# Connect and test connection\n",
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "\n",
        "es_url = os.environ['es_url']\n",
        "es_user = os.environ['es_user']\n",
        "es_pwd = os.environ['es_pwd']\n",
        "\n",
        "# Initialize the Elasticsearch client\n",
        "es = Elasticsearch(\n",
        "    [es_url],\n",
        "    basic_auth=(es_user, es_pwd),\n",
        "    request_timeout=30\n",
        ")\n",
        "es.info().body"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See https://registry.opendata.aws/amazon-pqa/\n",
        "# aws s3 ls --no-sign-request s3://amazon-pqa/\n",
        "\n",
        "# Upload the file first\n",
        "!head /content/sample_data/amazon_pqa_headset.json"
      ],
      "metadata": {
        "id": "y6e_aa9kyjcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data and Model Setup"
      ],
      "metadata": {
        "id": "ELvXvOSFZs56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data into the dataframe. 1000 rows for test\n",
        "import sys\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from elasticsearch import Elasticsearch\n",
        "from elasticsearch.helpers import bulk\n",
        "from datetime import datetime\n",
        "\n",
        "df = pd.DataFrame(columns=('question', 'answer'))\n",
        "\n",
        "with open('/content/sample_data/amazon_pqa_headset.json') as f:\n",
        "    i=0\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        df.loc[i] = [data['question_text'],data['answers'][0]['answer_text']]\n",
        "        i+=1\n",
        "        if(i == 1000):\n",
        "            break\n"
      ],
      "metadata": {
        "id": "ktH56r5qvOi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install eland\n",
        "!pip install -q eland elasticsearch elasticsearch_dsl transformers sentence_transformers\n",
        "\n",
        "from elasticsearch_dsl import Search\n",
        "from eland.ml.pytorch import PyTorchModel\n",
        "from eland.ml.pytorch.transformers import TransformerModel\n",
        "from getpass import getpass\n",
        "import logging\n",
        "import tempfile\n",
        "from pprint import pformat\n",
        "import secrets"
      ],
      "metadata": {
        "id": "9B5r8yVC6wQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the eland model\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "MODEL_HUB_URL = \"https://huggingface.co\"\n",
        "\n",
        "def load_model(model_id, task_type):\n",
        "  with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "    logger.info(f\"Loading HuggingFace transformer tokenizer and model [{model_id}] for task [{task_type}]\" )\n",
        "\n",
        "    tm = TransformerModel(model_id=model_id, task_type=task_type)\n",
        "    model_path, config, vocab_path = tm.save(tmp_dir)\n",
        "\n",
        "    ptm = PyTorchModel(es, tm.elasticsearch_model_id())\n",
        "    model_exists = es.options(ignore_status=404).ml.get_trained_models(model_id=ptm.model_id).meta.status == 200\n",
        "\n",
        "    if model_exists:\n",
        "      logger.info(\"Model has already been imported\")\n",
        "    else:\n",
        "      logger.info(\"Importing model\")\n",
        "      ptm.import_model(model_path=model_path, config_path=None, vocab_path=vocab_path, config=config)\n",
        "      logger.info(\"Starting model deployment\")\n",
        "      ptm.start()\n",
        "      logger.info(f\"Model successfully imported with id '{ptm.model_id}'\")"
      ],
      "metadata": {
        "id": "xJjODOJz6mgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_model(\"sentence-transformers/all-MiniLM-L12-v2\", \"text_embedding\")\n",
        "\n",
        "# fetch it so we can see how it loaded\n",
        "es.ml.get_trained_models(model_id=\"sentence-transformers__all-minilm-l12-v2\").body"
      ],
      "metadata": {
        "id": "VdGCezTY71AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Index, Pipeline and Load Index"
      ],
      "metadata": {
        "id": "FK2ferO_bBzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates an index in Elasticsearch if one isn't already there.\"\"\"\n",
        "es.options(ignore_status=400).indices.create(\n",
        "    index=\"nlp_pqa_1000\",\n",
        "    settings={\"number_of_shards\": 1},\n",
        "    mappings={\n",
        "        \"properties\": {\n",
        "            \"question\": { \"type\": \"text\"},\n",
        "            \"answer\": {\"type\": \"text\"},\n",
        "        }\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOHQYK38vvZh",
        "outputId": "54fd0881-c0b0-46b7-a1da-6621a5f28a5f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ObjectApiResponse({'error': {'root_cause': [{'type': 'resource_already_exists_exception', 'reason': 'index [nlp_pqa_1000/Bac8Vx1WTsKaGGR4-klrTg] already exists', 'index_uuid': 'Bac8Vx1WTsKaGGR4-klrTg', 'index': 'nlp_pqa_1000'}], 'type': 'resource_already_exists_exception', 'reason': 'index [nlp_pqa_1000/Bac8Vx1WTsKaGGR4-klrTg] already exists', 'index_uuid': 'Bac8Vx1WTsKaGGR4-klrTg', 'index': 'nlp_pqa_1000'}, 'status': 400})"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pipeline\n",
        "es.ingest.put_pipeline(id=\"sentence-text-embedding\",\n",
        "    description=\"Text embedding pipeline\",\n",
        "    processors=[\n",
        "    {\n",
        "        \"inference\": {\n",
        "        \"model_id\": \"sentence-transformers__all-minilm-l12-v2\",\n",
        "        \"field_map\": {\n",
        "            \"question_text\": \"text_field\"\n",
        "        },\n",
        "        \"target_field\": \"question_vector\"\n",
        "      }\n",
        "    }\n",
        "  ],\n",
        "  on_failure=[\n",
        "    {\n",
        "      \"set\": {\n",
        "        \"description\": \"Index document to 'failed-<index>'\",\n",
        "        \"field\": \"_index\",\n",
        "        \"value\": \"failed-{{{_index}}}\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"set\": {\n",
        "        \"description\": \"Set error message\",\n",
        "        \"field\": \"ingest.failure\",\n",
        "        \"value\": \"{{_ingest.on_failure_message}}\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "sO3ZPDrZCPcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es.options(ignore_status=400).indices.create(\n",
        "    index=\"nlp_pqa_1000_embeddings\",\n",
        "    settings={\"number_of_shards\": 1},\n",
        "    mappings={\n",
        "        \"properties\": {\n",
        "            \"question\": { \"type\": \"text\"},\n",
        "            \"answer\": {\"type\": \"text\"},\n",
        "            \"question_vector.predicted_value\": {\n",
        "            \"type\": \"dense_vector\",\n",
        "            \"dims\": 384,\n",
        "            \"index\": \"true\",\n",
        "            \"similarity\": \"cosine\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "80j3zYvcFcwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e1dc45-cb2b-4ac2-9566-230cf433f6d9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'nlp_pqa_1000_embeddings'})"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data through the model using a pipeline\n",
        "def generator():\n",
        "    for index, row in df.iterrows():\n",
        "        yield {\n",
        "            \"_index\": \"nlp_pqa_1000_embeddings\",\n",
        "            \"pipeline\": \"sentence-text-embedding\",\n",
        "            \"question_text\": row[\"question\"],\n",
        "            \"answer\": row[\"answer\"]\n",
        "        }\n",
        "\n",
        "try:\n",
        "    res = bulk(es, generator())\n",
        "    print(\"Response: \", res)\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "GhyIGRDzG_nW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca76a78-0400-452f-9385-cdc5dc33c994"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:  (1000, [])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the model if not started will error if started already\n",
        "es.ml.start_trained_model_deployment(model_id=\"sentence-transformers__all-minilm-l12-v2\")"
      ],
      "metadata": {
        "id": "_4prpkiIY05D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Queries"
      ],
      "metadata": {
        "id": "cRqzd4IOZ3I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple KNN\n",
        "query = input (\"Enter a question :\")\n",
        "print('\\n')\n",
        "\n",
        "knn = {\n",
        "    \"field\": \"question_vector.predicted_value\",\n",
        "    \"k\": 10,\n",
        "    \"num_candidates\": 100,\n",
        "    \"query_vector_builder\": {\n",
        "    \"text_embedding\": {\n",
        "        \"model_id\": \"sentence-transformers__all-minilm-l12-v2\",\n",
        "        \"model_text\": query\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "resp = es.search(index=\"nlp_pqa_1000_embeddings\", knn=knn)\n",
        "\n",
        "for hit in resp['hits']['hits']:\n",
        "    doc_id = hit['_id']\n",
        "    score = hit['_score']\n",
        "    question = hit['_source']['question_text']\n",
        "    answer = hit['_source']['answer']\n",
        "    print(f\"Score: {score}\\nQuestion: {question}\\nAnswer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "B3YXTTzzLUPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN with Filter\n",
        "\n",
        "query = input (\"Enter a question :\")\n",
        "print('\\n')\n",
        "\n",
        "knn = {\n",
        "    \"field\": \"question_vector.predicted_value\",\n",
        "    \"k\": 10,\n",
        "    \"num_candidates\": 100,\n",
        "    \"query_vector_builder\": {\n",
        "    \"text_embedding\": {\n",
        "        \"model_id\": \"sentence-transformers__all-minilm-l12-v2\",\n",
        "        \"model_text\": query\n",
        "        }\n",
        "    },\n",
        "    \"filter\": {\n",
        "      \"bool\": {\n",
        "        \"must_not\": [\n",
        "          {\n",
        "            \"match\": {\n",
        "              \"answer\": \"plantronics\"\n",
        "            }\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "resp = es.search(index=\"nlp_pqa_1000_embeddings\", knn=knn)\n",
        "\n",
        "for hit in resp['hits']['hits']:\n",
        "    doc_id = hit['_id']\n",
        "    score = hit['_score']\n",
        "    question = hit['_source']['question_text']\n",
        "    answer = hit['_source']['answer']\n",
        "    print(f\"Score: {score}\\nQuestion: {question}\\nAnswer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "Bm7a9_tBQ3gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_-CsI9BpZYky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid search rrf\n",
        "query = input (\"Enter a question :\")\n",
        "\n",
        "print('\\n')\n",
        "body = {\n",
        "    \"query\": {\n",
        "      \"bool\": {\n",
        "      \"must\": [\n",
        "        {\n",
        "          \"match\": {\n",
        "            \"answer\": \"polycom\"\n",
        "          }\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  },\n",
        "  \"knn\": {\n",
        "    \"field\": \"question_vector.predicted_value\",\n",
        "    \"k\": 10,\n",
        "    \"num_candidates\": 100,\n",
        "    \"query_vector_builder\": {\n",
        "      \"text_embedding\": {\n",
        "        \"model_id\": \"sentence-transformers__all-minilm-l12-v2\",\n",
        "        \"model_text\": query\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"rank\": {\n",
        "        \"rrf\": {\n",
        "            \"window_size\": 50,\n",
        "            \"rank_constant\": 20\n",
        "        }\n",
        "    }\n",
        "}\n",
        "resp = es.search(index=\"nlp_pqa_1000_embeddings\", body=body)\n",
        "\n",
        "for hit in resp['hits']['hits']:\n",
        "    doc_id = hit['_id']\n",
        "    rank = hit['_rank']\n",
        "    question = hit['_source']['question_text']\n",
        "    answer = hit['_source']['answer']\n",
        "    print(f\"\\nRank: {rank}\\nQuestion: {question}\\nAnswer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "KuwwDu3p8BWS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}