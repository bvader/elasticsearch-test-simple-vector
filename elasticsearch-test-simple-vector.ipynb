{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bvader/elasticsearch-test-simple-vector/blob/main/elasticsearch-test-simple-vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "ObVXyocDZSZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install elasticsearch\n"
      ],
      "metadata": {
        "id": "cRUzDQ2MtIna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in connection and auth info\n",
        "# Note the port is REQUIRED for the elasticsearch endpoint!\n",
        "import getpass, os\n",
        "\n",
        "os.environ['es_url'] = getpass.getpass('Enter Elasticsearch Endpoint:  ')\n",
        "os.environ['es_user'] = getpass.getpass('Enter User:  ')\n",
        "os.environ['es_pwd'] = getpass.getpass('Enter Password:  ')"
      ],
      "metadata": {
        "id": "Y3umJ1oqT4SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "XbqVOuPbsxb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b4fa9ac-3221-4959-b90b-78352c835555"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'instance-0000000001',\n",
              " 'cluster_name': '44feffcc909849f295ae3ed4a9be10c1',\n",
              " 'cluster_uuid': 'Fm7mp0U6S0y1ASR9Gjvh3w',\n",
              " 'version': {'number': '8.9.0',\n",
              "  'build_flavor': 'default',\n",
              "  'build_type': 'docker',\n",
              "  'build_hash': '8aa461beb06aa0417a231c345a1b8c38fb498a0d',\n",
              "  'build_date': '2023-07-19T14:43:58.555259655Z',\n",
              "  'build_snapshot': False,\n",
              "  'lucene_version': '9.7.0',\n",
              "  'minimum_wire_compatibility_version': '7.17.0',\n",
              "  'minimum_index_compatibility_version': '7.0.0'},\n",
              " 'tagline': 'You Know, for Search'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# Connect and test connection\n",
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "\n",
        "es_url = os.environ['es_url']\n",
        "es_user = os.environ['es_user']\n",
        "es_pwd = os.environ['es_pwd']\n",
        "\n",
        "# Initialize the Elasticsearch client\n",
        "es = Elasticsearch(\n",
        "    [es_url],\n",
        "    basic_auth=(es_user, es_pwd),\n",
        "    request_timeout=30\n",
        ")\n",
        "es.info().body"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the file first\n",
        "!head /content/sample_data/amazon_pqa_headset.json"
      ],
      "metadata": {
        "id": "y6e_aa9kyjcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data and Model Setup"
      ],
      "metadata": {
        "id": "ELvXvOSFZs56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data into the dataframe. 1000 rows for test\n",
        "import sys\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from elasticsearch import Elasticsearch\n",
        "from elasticsearch.helpers import bulk\n",
        "from datetime import datetime\n",
        "\n",
        "df = pd.DataFrame(columns=('question', 'answer'))\n",
        "\n",
        "with open('/content/sample_data/amazon_pqa_headset.json') as f:\n",
        "    i=0\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        df.loc[i] = [data['question_text'],data['answers'][0]['answer_text']]\n",
        "        i+=1\n",
        "        if(i == 1000):\n",
        "            break\n"
      ],
      "metadata": {
        "id": "ktH56r5qvOi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install eland\n",
        "!pip install -q eland elasticsearch elasticsearch_dsl transformers sentence_transformers\n",
        "\n",
        "from elasticsearch_dsl import Search\n",
        "from eland.ml.pytorch import PyTorchModel\n",
        "from eland.ml.pytorch.transformers import TransformerModel\n",
        "from getpass import getpass\n",
        "import logging\n",
        "import tempfile\n",
        "from pprint import pformat\n",
        "import secrets"
      ],
      "metadata": {
        "id": "9B5r8yVC6wQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the eland model\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "MODEL_HUB_URL = \"https://huggingface.co\"\n",
        "\n",
        "def load_model(model_id, task_type):\n",
        "  with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "    logger.info(f\"Loading HuggingFace transformer tokenizer and model [{model_id}] for task [{task_type}]\" )\n",
        "\n",
        "    tm = TransformerModel(model_id=model_id, task_type=task_type)\n",
        "    model_path, config, vocab_path = tm.save(tmp_dir)\n",
        "\n",
        "    ptm = PyTorchModel(es, tm.elasticsearch_model_id())\n",
        "    model_exists = es.options(ignore_status=404).ml.get_trained_models(model_id=ptm.model_id).meta.status == 200\n",
        "\n",
        "    if model_exists:\n",
        "      logger.info(\"Model has already been imported\")\n",
        "    else:\n",
        "      logger.info(\"Importing model\")\n",
        "      ptm.import_model(model_path=model_path, config_path=None, vocab_path=vocab_path, config=config)\n",
        "      logger.info(\"Starting model deployment\")\n",
        "      ptm.start()\n",
        "      logger.info(f\"Model successfully imported with id '{ptm.model_id}'\")"
      ],
      "metadata": {
        "id": "xJjODOJz6mgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_model(\"sentence-transformers/all-MiniLM-L12-v2\", \"text_embedding\")\n",
        "\n",
        "# fetch it so we can see how it loaded\n",
        "es.ml.get_trained_models(model_id=\"sentence-transformers__all-minilm-l12-v2\").body"
      ],
      "metadata": {
        "id": "VdGCezTY71AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Index, Pipeline and Load Index"
      ],
      "metadata": {
        "id": "FK2ferO_bBzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates an index in Elasticsearch if one isn't already there.\"\"\"\n",
        "es.options(ignore_status=400).indices.create(\n",
        "    index=\"nlp_pqa_1000\",\n",
        "    settings={\"number_of_shards\": 1},\n",
        "    mappings={\n",
        "        \"properties\": {\n",
        "            \"question\": { \"type\": \"text\"},\n",
        "            \"answer\": {\"type\": \"text\"},\n",
        "        }\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOHQYK38vvZh",
        "outputId": "54fd0881-c0b0-46b7-a1da-6621a5f28a5f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ObjectApiResponse({'error': {'root_cause': [{'type': 'resource_already_exists_exception', 'reason': 'index [nlp_pqa_1000/Bac8Vx1WTsKaGGR4-klrTg] already exists', 'index_uuid': 'Bac8Vx1WTsKaGGR4-klrTg', 'index': 'nlp_pqa_1000'}], 'type': 'resource_already_exists_exception', 'reason': 'index [nlp_pqa_1000/Bac8Vx1WTsKaGGR4-klrTg] already exists', 'index_uuid': 'Bac8Vx1WTsKaGGR4-klrTg', 'index': 'nlp_pqa_1000'}, 'status': 400})"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pipeline\n",
        "es.ingest.put_pipeline(id=\"sentence-text-embedding\",\n",
        "    description=\"Text embedding pipeline\",\n",
        "    processors=[\n",
        "    {\n",
        "        \"inference\": {\n",
        "        \"model_id\": \"sentence-transformers__all-minilm-l12-v2\",\n",
        "        \"field_map\": {\n",
        "            \"question_text\": \"text_field\"\n",
        "        },\n",
        "        \"target_field\": \"question_vector\"\n",
        "      }\n",
        "    }\n",
        "  ],\n",
        "  on_failure=[\n",
        "    {\n",
        "      \"set\": {\n",
        "        \"description\": \"Index document to 'failed-<index>'\",\n",
        "        \"field\": \"_index\",\n",
        "        \"value\": \"failed-{{{_index}}}\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"set\": {\n",
        "        \"description\": \"Set error message\",\n",
        "        \"field\": \"ingest.failure\",\n",
        "        \"value\": \"{{_ingest.on_failure_message}}\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "sO3ZPDrZCPcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es.options(ignore_status=400).indices.create(\n",
        "    index=\"nlp_pqa_1000_embeddings\",\n",
        "    settings={\"number_of_shards\": 1},\n",
        "    mappings={\n",
        "        \"properties\": {\n",
        "            \"question\": { \"type\": \"text\"},\n",
        "            \"answer\": {\"type\": \"text\"},\n",
        "            \"question_vector.predicted_value\": {\n",
        "            \"type\": \"dense_vector\",\n",
        "            \"dims\": 384,\n",
        "            \"index\": \"true\",\n",
        "            \"similarity\": \"cosine\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "80j3zYvcFcwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e1dc45-cb2b-4ac2-9566-230cf433f6d9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'nlp_pqa_1000_embeddings'})"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data through the model using a pipeline\n",
        "def generator():\n",
        "    for index, row in df.iterrows():\n",
        "        yield {\n",
        "            \"_index\": \"nlp_pqa_1000_embeddings\",\n",
        "            \"pipeline\": \"sentence-text-embedding\",\n",
        "            \"question_text\": row[\"question\"],\n",
        "            \"answer\": row[\"answer\"]\n",
        "        }\n",
        "\n",
        "try:\n",
        "    res = bulk(es, generator())\n",
        "    print(\"Response: \", res)\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "GhyIGRDzG_nW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca76a78-0400-452f-9385-cdc5dc33c994"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:  (1000, [])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the model if not started will error if started already\n",
        "es.ml.start_trained_model_deployment(model_id=\"sentence-transformers__all-minilm-l12-v2\")"
      ],
      "metadata": {
        "id": "_4prpkiIY05D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Queries"
      ],
      "metadata": {
        "id": "cRqzd4IOZ3I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple KNN\n",
        "query = input (\"Enter a question :\")\n",
        "print('\\n')\n",
        "\n",
        "knn = {\n",
        "    \"field\": \"question_vector.predicted_value\",\n",
        "    \"k\": 10,\n",
        "    \"num_candidates\": 100,\n",
        "    \"query_vector_builder\": {\n",
        "    \"text_embedding\": {\n",
        "        \"model_id\": \"sentence-transformers__all-minilm-l12-v2\",\n",
        "        \"model_text\": query\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "resp = es.search(index=\"nlp_pqa_1000_embeddings\", knn=knn)\n",
        "\n",
        "for hit in resp['hits']['hits']:\n",
        "    doc_id = hit['_id']\n",
        "    score = hit['_score']\n",
        "    question = hit['_source']['question_text']\n",
        "    answer = hit['_source']['answer']\n",
        "    print(f\"Score: {score}\\nQuestion: {question}\\nAnswer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "B3YXTTzzLUPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN with Filter\n",
        "\n",
        "query = input (\"Enter a question :\")\n",
        "print('\\n')\n",
        "\n",
        "knn = {\n",
        "    \"field\": \"question_vector.predicted_value\",\n",
        "    \"k\": 10,\n",
        "    \"num_candidates\": 100,\n",
        "    \"query_vector_builder\": {\n",
        "    \"text_embedding\": {\n",
        "        \"model_id\": \"sentence-transformers__all-minilm-l12-v2\",\n",
        "        \"model_text\": query\n",
        "        }\n",
        "    },\n",
        "    \"filter\": {\n",
        "      \"bool\": {\n",
        "        \"must_not\": [\n",
        "          {\n",
        "            \"match\": {\n",
        "              \"answer\": \"plantronics\"\n",
        "            }\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "resp = es.search(index=\"nlp_pqa_1000_embeddings\", knn=knn)\n",
        "\n",
        "for hit in resp['hits']['hits']:\n",
        "    doc_id = hit['_id']\n",
        "    score = hit['_score']\n",
        "    question = hit['_source']['question_text']\n",
        "    answer = hit['_source']['answer']\n",
        "    print(f\"Score: {score}\\nQuestion: {question}\\nAnswer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "Bm7a9_tBQ3gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_-CsI9BpZYky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid search rrf\n",
        "query = input (\"Enter a question :\")\n",
        "\n",
        "print('\\n')\n",
        "body = {\n",
        "    \"query\": {\n",
        "      \"bool\": {\n",
        "      \"must\": [\n",
        "        {\n",
        "          \"match\": {\n",
        "            \"answer\": \"polycom\"\n",
        "          }\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  },\n",
        "  \"knn\": {\n",
        "    \"field\": \"question_vector.predicted_value\",\n",
        "    \"k\": 10,\n",
        "    \"num_candidates\": 100,\n",
        "    \"query_vector_builder\": {\n",
        "      \"text_embedding\": {\n",
        "        \"model_id\": \"sentence-transformers__all-minilm-l12-v2\",\n",
        "        \"model_text\": query\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"rank\": {\n",
        "        \"rrf\": {\n",
        "            \"window_size\": 50,\n",
        "            \"rank_constant\": 20\n",
        "        }\n",
        "    }\n",
        "}\n",
        "resp = es.search(index=\"nlp_pqa_1000_embeddings\", body=body)\n",
        "\n",
        "for hit in resp['hits']['hits']:\n",
        "    doc_id = hit['_id']\n",
        "    rank = hit['_rank']\n",
        "    question = hit['_source']['question_text']\n",
        "    answer = hit['_source']['answer']\n",
        "    print(f\"\\nRank: {rank}\\nQuestion: {question}\\nAnswer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "KuwwDu3p8BWS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}